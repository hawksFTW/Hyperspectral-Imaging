{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"datasets.ipynb","provenance":[],"authorship_tag":"ABX9TyO7gZBBqsvz5CHyZFQ7k7dV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LjdTpOWLumy","executionInfo":{"status":"ok","timestamp":1643483076674,"user_tz":480,"elapsed":4456,"user":{"displayName":"hawksFTW","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10550048566476126053"}},"outputId":"12aab198-d6bd-471b-8e71-ef414618f66c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: hdf5storage in /usr/local/lib/python3.7/dist-packages (0.1.18)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (1.19.5)\n","Requirement already satisfied: h5py>=2.1 in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (3.1.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.1->hdf5storage) (1.5.2)\n"]}],"source":["# import the necessary packages\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.preprocessing import MinMaxScaler\n","import pandas as pd\n","import numpy as np\n","import glob\n","import cv2\n","import os\n","\n","import argparse\n","import os\n","import torch\n","import numpy as np\n","import cv2\n","!pip install hdf5storage\n","import hdf5storage as hdf5\n","import sys\n","\n","from scipy.io import loadmat\n","import pandas as pd"]},{"cell_type":"code","source":["def load_firmness_attributes(inputPath):\n","\t# initialize the list of column names in the CSV file and then\n","\t# load it using Pandas\n","\n","\tcols = [\"fid\", \"firmness\", \"texture\", \"fungus\", \"weight\"]\n","\tdf = pd.read_csv(inputPath, sep=\",\", header=0, names=cols)\n","\n","\t# determine (1) the unique zip codes and (2) the number of data\n","\t# points with each zip code\n","\tdf = df.set_index(\"fid\")\n","\n","    # return the data frame\n","\treturn df\n","\n","\n","def load_produce_images(df, inputPath):\n","    inputImages = []\n","    for i in df.index.values:\n","        spectral = hdf5.loadmat(inputPath + i + \".jpeg\")\n","        img = spectral[\"cube\"]\n","        img = cv2.resize(img, (256, 256))\n","        inputImages.append(img)\n","        \n","    return (np.array(inputImages))\n","\n","\n","def process_attributes(df, train, test):\n","    continuous = [\"firmness\"]\n","    cs = MinMaxScaler()\n","    trainContinuous = cs.fit_transform(train[continuous])\n","    testContinuous = cs.transform(test[continuous])\n","    trainX = np.hstack([trainContinuous])\n","    testX = np.hstack([testContinuous])\n","    \n","    return(trainX, testX)"],"metadata":{"id":"DAgXOXvFMDCD","executionInfo":{"status":"ok","timestamp":1643482514369,"user_tz":480,"elapsed":207,"user":{"displayName":"hawksFTW","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10550048566476126053"}}},"execution_count":3,"outputs":[]}]}